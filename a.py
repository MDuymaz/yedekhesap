# dizipal_episodes_m3u.py
# Gereksinim: pip install cloudscraper beautifulsoup4
import time
import json
import re
import cloudscraper
from bs4 import BeautifulSoup

# -------------------------
# Ayarlar / sabitler
# -------------------------
DOMAIN_URL = "https://raw.githubusercontent.com/zerodayip/domain/refs/heads/main/dizipal.txt"
JSON_FILE = "dizipal/diziler.json"
OUTPUT_FILE = "dizipalyerlidizi.m3u"

# KaÃ§ karakter HTML'i yazdÄ±rmak istersin (Ã§ok uzunsa terminali doldurabilir)
HTML_PRINT_LIMIT = 2000

# Her sayfa isteÄŸi iÃ§in en fazla kaÃ§ deneme yapÄ±lsÄ±n
MAX_RETRIES = 3
# Ä°stek zaman aÅŸÄ±mÄ± (saniye)
REQUEST_TIMEOUT = 15

# Cloudscraper oluÅŸtur
scraper = cloudscraper.create_scraper()

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/121.0.0.0 Safari/537.36",
    # Referer dinamik olarak BASE_URL olarak ayarlanacak
}

# -------------------------
# YardÄ±mcÄ± fonksiyonlar
# -------------------------
def fetch_text_with_retries(url, headers=None, timeout=REQUEST_TIMEOUT, max_retries=MAX_RETRIES):
    """
    Verilen URL'den cloudscraper ile GET yapar. BaÅŸarÄ±sÄ±z olursa birkaÃ§ kez tekrar dener.
    DÃ¶nen response.text'i ve status_code'u tuple olarak dÃ¶ndÃ¼rÃ¼r.
    """
    attempt = 0
    wait_seconds = 2
    last_resp = None

    while attempt < max_retries:
        attempt += 1
        try:
            resp = scraper.get(url, headers=headers or {}, timeout=timeout)
            last_resp = resp
            # Her denemede status code'u kontrol et; 200 gelirse RETURN et
            if resp.status_code == 200:
                return resp.text, resp.status_code
            else:
                # 403 veya 503 gibi durumlarda challenge sayfasÄ± gelebilir, HTML'i incelemek iÃ§in dÃ¶ndÃ¼rmeden Ã¶nce loglayacaÄŸÄ±z
                print(f"âš ï¸ Deneme {attempt}/{max_retries}: HTTP {resp.status_code} dÃ¶ndÃ¼ for {url}.")
                # EÄŸer halen deneme hakkÄ± varsa bekle ve tekrar dene
                if attempt < max_retries:
                    time.sleep(wait_seconds)
                    wait_seconds *= 2
                else:
                    return resp.text, resp.status_code
        except Exception as e:
            print(f"âš ï¸ Deneme {attempt}/{max_retries} sÄ±rasÄ±nda istisna oluÅŸtu: {e}")
            if attempt < max_retries:
                time.sleep(wait_seconds)
                wait_seconds *= 2
            else:
                raise

    # EÄŸer hiÃ§ dÃ¶nemediysek son resp veya None dÃ¶nebilir
    if last_resp:
        return last_resp.text, last_resp.status_code
    return None, None

def scrape_series_episodes(series_href, base_url, headers):
    """
    Tek bir dizi sayfasÄ±nÄ± Ã§eker, HTML'i yazdÄ±rÄ±r ve bÃ¶lÃ¼m listesini + IMDb puanÄ±nÄ± dÃ¶ndÃ¼rÃ¼r.
    """
    url = f"{base_url}{series_href}"
    html_text, status = fetch_text_with_retries(url, headers=headers)

    # Gelen HTML'i her durumda yazdÄ±r (kÄ±smÃ®)
    separator = "-" * 40
    print(f"\nğŸ“„ {url} sayfasÄ±nÄ±n HTML (ilk {HTML_PRINT_LIMIT} karakter):\n{separator}")
    if html_text is None:
        print("âš ï¸ HiÃ§bir cevap alÄ±namadÄ± (html_text is None).")
    else:
        # GÃ¼venlik: terminali aÅŸÄ±rÄ± doldurmamak iÃ§in limitli yazdÄ±r
        print(html_text[:HTML_PRINT_LIMIT])
    print(f"\n{separator}\nHTTP status: {status}\n")

    # EÄŸer 200 deÄŸilse hata fÄ±rlat
    if status != 200:
        raise Exception(f"Sayfa aÃ§Ä±lamadÄ±! HTTP {status}")

    soup = BeautifulSoup(html_text, "html.parser")

    # IMDb puanÄ±nÄ± al
    imdb_div = soup.find("div", class_="key", string=re.compile(r"IMDB", re.I))
    if imdb_div:
        value_div = imdb_div.find_next_sibling("div", class_="value")
        imdb_score = value_div.get_text(strip=True) if value_div else "-"
    else:
        imdb_score = "-"

    # BÃ¶lÃ¼mleri al (site yapÄ±sÄ±na gÃ¶re seÃ§ici)
    episodes = []
    # EÄŸer site farklÄ± bir sÄ±nÄ±f kullanÄ±yorsa burayÄ± deÄŸiÅŸtirebilirsin
    for ep_div in soup.select("div.episode-item a[href]"):
        ep_href = ep_div.get("href")
        ep_title_div = ep_div.select_one("div.episode")
        ep_title = ep_title_div.get_text(strip=True) if ep_title_div else "-"
        episodes.append({"href": ep_href, "title": ep_title})

    # EÄŸer yukarÄ±daki seÃ§ici boÅŸ dÃ¶nerse alternatif olarak baÄŸlantÄ±larÄ± gÃ¶zetleyelim
    if not episodes:
        for a in soup.select("a[href]"):
            # Ã¶rnek: href iÃ§inde '/dizi/' ve 'bolum' gibi pattern'ler aranabilir
            href = a.get("href")
            txt = a.get_text(strip=True) or ""
            if href and "/bolum" in href.lower() or "bÃ¶lÃ¼m" in txt.lower():
                episodes.append({"href": href, "title": txt or href})

    return {"imdb": imdb_score, "episodes": episodes}

# -------------------------
# Ana akÄ±ÅŸ
# -------------------------
def main():
    # 1) Github Ã¼zerinden domain al
    print("ğŸŒ Domain bilgisi alÄ±nÄ±yor...")
    domain_text, domain_status = fetch_text_with_retries(DOMAIN_URL, headers=HEADERS)
    if domain_text is None or domain_status != 200:
        raise Exception(f"Domain alÄ±namadÄ±! HTTP {domain_status}")

    BASE_URL = domain_text.strip()
    print(f"ğŸŒ KullanÄ±lan BASE_URL: {BASE_URL}")

    # Referer header'Ä±nÄ± BASE_URL olarak gÃ¼ncelle
    HEADERS["Referer"] = BASE_URL

    # 2) Ana sayfa yÃ¼klenmesi iÃ§in bekleme
    print("â³ Ana sayfa yÃ¼kleniyor, 30 saniye bekleniyor...", flush=True)
    time.sleep(30)

    # 3) JSON dosyasÄ±nÄ± oku
    with open(JSON_FILE, "r", encoding="utf-8") as f:
        series_data = json.load(f)

    # 4) m3u dosyasÄ±nÄ± oluÅŸtur
    with open(OUTPUT_FILE, "w", encoding="utf-8") as m3u_file:
        print("#EXTM3U", flush=True, file=m3u_file)

        for series_href, info in series_data.items():
            # group deÄŸiÅŸkenini try dÄ±ÅŸÄ±nda tanÄ±mlÄ±yoruz (hata sÄ±rasÄ±nda da kullanÄ±labilsin)
            group = info.get("group", "UNKNOWN")
            tvg_logo = info.get("tvg-logo", "")
            print(f"ğŸ¬ {group} bÃ¶lÃ¼mleri Ã§ekiliyor.", flush=True)

            try:
                data = scrape_series_episodes(series_href, BASE_URL, HEADERS)
                imdb_score = data.get("imdb", "-")

                for ep in data.get("episodes", []):
                    ep_href = ep.get("href")
                    ep_title_full = (ep.get("title") or "").upper()

                    # Sezon ve bÃ¶lÃ¼m numarasÄ±nÄ± ayÄ±kla
                    season_match = re.search(r"(\d+)\.\s*SEZON", ep_title_full)
                    episode_match = re.search(r"(\d+)\.\s*BÃ–LÃœM", ep_title_full)
                    season = season_match.group(1).zfill(2) if season_match else "01"
                    episode = episode_match.group(1).zfill(2) if episode_match else "01"

                    tvg_name = f"{group.upper()} S{season}E{episode}"
                    extinf_line = (
                        f'#EXTINF:-1 tvg-id="" tvg-name="{tvg_name.upper()}" '
                        f'tvg-logo="{tvg_logo}" group-title="{group.upper()}", '
                        f'{group.upper()} {season}. SEZON {episode} (IMDb: {imdb_score} | YERLÄ° DÄ°ZÄ° | DIZIPAL)'
                    )

                    # proxy url (eski mantÄ±ÄŸÄ±nla aynÄ±)
                    proxy_url = f"https://zerodayip.com/proxy/dizipal?url={BASE_URL}{ep_href}"

                    # Dosyaya yaz
                    print(extinf_line, file=m3u_file, flush=True)
                    print(proxy_url, file=m3u_file, flush=True)

            except Exception as e:
                # Hata mesajÄ±nda group kullanÄ±mÄ± gÃ¼venli (try dÄ±ÅŸÄ±nda tanÄ±mlÄ±)
                print(f"âš ï¸ {group} iÃ§in hata: {e}", flush=True)

    print(f"\nâœ… Dizipal m3u dosyasÄ± hazÄ±rlandÄ±: {OUTPUT_FILE}", flush=True)

if __name__ == "__main__":
    main()
